{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-02T08:09:50.789468Z",
     "start_time": "2018-10-02T08:09:50.713799Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import time\n",
    "time.sleep(8*3600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## detecting and cropping faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-02T08:09:52.814508Z",
     "start_time": "2018-10-02T08:09:50.811380Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to load cuda, but it is not available\n",
      "no cuda? True\n",
      "Found 1117 sample images;  894  to train 223 to test\n"
     ]
    }
   ],
   "source": [
    "done = False\n",
    "done = True\n",
    "from gaze import init, Data\n",
    "args = init(batch_size=8, verbose=1)\n",
    "d = Data(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-02T08:09:52.937667Z",
     "start_time": "2018-10-02T08:09:52.831771Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/blink/2018-09-05_Laurent_090.png\n",
      "dataset/blink/2018-09-12_Laurent_035.png\n",
      "dataset/blink/2018-09-12_Laurent_223.png\n",
      "dataset/blink/2018-09-12_Laurent_237.png\n",
      "dataset/blink/2018-09-12_Laurent_222.png\n",
      "dataset/blink/2018-09-14_Laurent_121.png\n",
      "dataset/blink/2018-09-04_Laurent_180.png\n",
      "dataset/blink/2018-09-12_Laurent_020.png\n",
      "dataset/blink/2018-09-05_Laurent_085.png\n",
      "dataset/blink/2018-09-14_Laurent_069.png\n",
      "dataset/center/2018-09-05_Laurent_244.png\n",
      "dataset/center/2018-09-05_Laurent_091.png\n",
      "dataset/center/2018-09-04_Laurent_16.png\n",
      "dataset/center/2018-09-12_Laurent_140.png\n",
      "dataset/center/2018-09-14_Laurent_243.png\n",
      "dataset/center/2018-09-12_Laurent_168.png\n",
      "dataset/center/2018-09-12_Laurent_183.png\n",
      "dataset/center/2018-09-04_Laurent_221.png\n",
      "dataset/center/2018-09-04_Laurent_235.png\n",
      "dataset/center/2018-09-14_Laurent_096.png\n",
      "dataset/left/2018-09-14_Laurent_108.png\n",
      "dataset/left/2018-09-12_Laurent_236.png\n",
      "dataset/left/2018-09-05_Laurent_091.png\n",
      "dataset/left/2018-09-12_Laurent_008.png\n",
      "dataset/left/2018-09-05_Laurent_052.png\n",
      "dataset/left/2018-09-12_Laurent_154.png\n",
      "dataset/left/2018-09-04_Laurent_209.png\n",
      "dataset/left/2018-09-05_Laurent_126.png\n",
      "dataset/left/2018-09-05_Laurent_132.png\n",
      "dataset/left/2018-09-14_Laurent_041.png\n",
      "dataset/right/2018-09-04_Laurent_195.png\n",
      "dataset/right/2018-09-05_Laurent_250.png\n",
      "dataset/right/2018-09-04_Laurent_143.png\n",
      "dataset/right/2018-09-12_Laurent_034.png\n",
      "dataset/right/2018-09-05_Laurent_046.png\n",
      "dataset/right/2018-09-12_Laurent_197.png\n",
      "dataset/right/2018-09-05_Laurent_126.png\n",
      "dataset/right/2018-09-05_Laurent_132.png\n",
      "dataset/right/2018-09-14_Laurent_055.png\n",
      "dataset/right/2018-09-14_Laurent_082.png\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "datapath_faces = 'dataset_faces'\n",
    "datapath = 'dataset'\n",
    "\n",
    "if True:\n",
    "    n_show = 10\n",
    "    for target in d.classes:\n",
    "        for filename in glob.glob(os.path.join(datapath, target) + '/*.png')[-n_show:]:\n",
    "            print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-02T08:09:53.720660Z",
     "start_time": "2018-10-02T08:09:52.952133Z"
    }
   },
   "outputs": [],
   "source": [
    "import imageio\n",
    "try:\n",
    "    os.mkdir(datapath_faces)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "for target in d.classes:\n",
    "    try:\n",
    "        os.mkdir(os.path.join(datapath_faces, target))\n",
    "        print('Creating folder ', os.path.join(datapath_faces, target))\n",
    "    except:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-02T08:09:53.824221Z",
     "start_time": "2018-10-02T08:09:53.734720Z"
    }
   },
   "outputs": [],
   "source": [
    "if not done:\n",
    "    import dlib\n",
    "    detector = dlib.get_frontal_face_detector()\n",
    "\n",
    "    for target in d.classes:\n",
    "        for filename in glob.glob(os.path.join(datapath, target) + '/*.png'):\n",
    "            frame = imageio.imread(filename)\n",
    "            N_X, N_Y, three = frame.shape\n",
    "            dets = detector(frame, 1)\n",
    "            if False:\n",
    "                print(\"Number of faces detected: {}\".format(len(dets)))\n",
    "                for i, d in enumerate(dets):\n",
    "                    print(\"Detection {}: Left: {} Top: {} Right: {} Bottom: {}\".format(\n",
    "                        i, d.left(), d.top(), d.right(), d.bottom()))\n",
    "\n",
    "            d = dets[0]\n",
    "            if False:\n",
    "                # Create figure and axes\n",
    "                fig, ax = plt.subplots(figsize=(15, 8))\n",
    "\n",
    "                # Display the image\n",
    "                ax.imshow(frame)\n",
    "\n",
    "                # Create a Rectangle patch\n",
    "                rect = patches.Rectangle((d.bottom(), d.left()), d.right()-d.left(), d.top()-d.bottom(), linewidth=1, edgecolor='r', facecolor='none')\n",
    "\n",
    "                # Add the patch to the Axes\n",
    "                ax.add_patch(rect)\n",
    "                plt.show()    \n",
    "            if False:\n",
    "                # Create figure and axes\n",
    "                fig, ax = plt.subplots(figsize=(15, 8))\n",
    "\n",
    "                # Display the cropped image\n",
    "                ax.imshow(frame[(d.top()):(d.bottom()), (d.left()):(d.right()), :])\n",
    "\n",
    "                plt.show()                \n",
    "\n",
    "            filename_face = filename.replace(datapath, datapath_faces)\n",
    "            imageio.imwrite(filename_face, frame[(d.top()):(d.bottom()), (d.left()):(d.right()), :]) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## running the model on the cropped faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-02T08:09:53.910204Z",
     "start_time": "2018-10-02T08:09:53.838793Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters= {'dataset_folder': 'dataset_faces', 'batch_size': 8, 'test_batch_size': 1, 'valid_size': 0.2, 'epochs': 40, 'do_adam': False, 'lr': 0.025, 'momentum': 0.05, 'no_cuda': False, 'num_processes': 1, 'seed': 42, 'log_interval': 0, 'fullsize': 64, 'crop': 64, 'size': 64, 'mean': 0.36, 'std': 0.3, 'conv1_dim': 9, 'conv1_kernel_size': 18, 'conv2_dim': 36, 'conv2_kernel_size': 14, 'stride1': 2, 'stride2': 4, 'N_cv': 20, 'dimension': 30, 'verbose': 0}\n"
     ]
    }
   ],
   "source": [
    "from gaze import init\n",
    "epochs = 40\n",
    "args = init(verbose=0, epochs=epochs)\n",
    "print('Parameters=', args)\n",
    "\n",
    "path = '_Regard.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-02T08:09:54.202874Z",
     "start_time": "2018-10-02T08:09:53.924361Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--  1 laurentperrinet  staff  100741 Sep 13 07:13 _Regard.pt\r\n"
     ]
    }
   ],
   "source": [
    "!ls -l {path}\n",
    "#!rm {path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-02T08:09:54.648770Z",
     "start_time": "2018-10-02T08:09:54.218490Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for Net:\n\tsize mismatch for conv1.weight: copying a param of torch.Size([9, 3, 18, 18]) from checkpoint, where the shape is torch.Size([9, 3, 11, 11]) in current model.\n\tsize mismatch for conv2.weight: copying a param of torch.Size([36, 9, 14, 14]) from checkpoint, where the shape is torch.Size([18, 9, 7, 7]) in current model.\n\tsize mismatch for conv2.bias: copying a param of torch.Size([36]) from checkpoint, where the shape is torch.Size([18]) in current model.\n\tsize mismatch for fc1.weight: copying a param of torch.Size([30, 144]) from checkpoint, where the shape is torch.Size([30, 450]) in current model.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-17291e5b209a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loading file'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0;32m--> 723\u001b[0;31m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[1;32m    724\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_named_members\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_members_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Net:\n\tsize mismatch for conv1.weight: copying a param of torch.Size([9, 3, 18, 18]) from checkpoint, where the shape is torch.Size([9, 3, 11, 11]) in current model.\n\tsize mismatch for conv2.weight: copying a param of torch.Size([36, 9, 14, 14]) from checkpoint, where the shape is torch.Size([18, 9, 7, 7]) in current model.\n\tsize mismatch for conv2.bias: copying a param of torch.Size([36]) from checkpoint, where the shape is torch.Size([18]) in current model.\n\tsize mismatch for fc1.weight: copying a param of torch.Size([30, 144]) from checkpoint, where the shape is torch.Size([30, 450]) in current model."
     ]
    }
   ],
   "source": [
    "from gaze import ML\n",
    "ml = ML(args)\n",
    "\n",
    "import os\n",
    "import torch\n",
    "if os.path.isfile(path):\n",
    "    ml.model.load_state_dict(torch.load(path))\n",
    "    print('Loading file', path)\n",
    "else:\n",
    "    print('Training model...')\n",
    "    ml.train()\n",
    "    torch.save(ml.model.state_dict(), path) #save the neural network state\n",
    "    print('Model saved at', path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-02T08:09:54.665290Z",
     "start_time": "2018-10-02T08:09:50.798Z"
    }
   },
   "outputs": [],
   "source": [
    "Accuracy = ml.test()\n",
    "print('Accuracy={:.1f}%'.format(Accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Showing the one which are wrong:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-02T08:09:54.683458Z",
     "start_time": "2018-10-02T08:09:50.820Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for data, labels in ml.dataset.test_loader:\n",
    "    fig, ax = ml.show(only_wrong=True)\n",
    "    plt.show() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-02T08:09:54.695901Z",
     "start_time": "2018-10-02T08:09:50.839Z"
    }
   },
   "outputs": [],
   "source": [
    "args = init(verbose=0, log_interval=0, epochs=20)\n",
    "from gaze import MetaML\n",
    "mml = MetaML(args)\n",
    "Accuracy = mml.protocol(args, 42)\n",
    "print('Accuracy', Accuracy[:-1].mean(), '+/-', Accuracy[:-1].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-02T08:09:54.704021Z",
     "start_time": "2018-10-02T08:09:50.891Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "fig, ax = plt.subplots(figsize=((8, 5)))\n",
    "n, bins, patches = ax.hist(Accuracy[:-1]*100, bins=np.linspace(0, 100, 100), alpha=.4)\n",
    "ax.vlines(np.median(Accuracy[:-1])*100, 0, n.max(), 'g', linestyles='dashed', label='median')\n",
    "ax.vlines(25, 0, n.max(), 'r', linestyles='dashed', label='chance level')\n",
    "ax.vlines(100, 0, n.max(), 'k', label='max')\n",
    "ax.set_xlabel('Accuracy (%)')\n",
    "ax.set_ylabel('Smarts')\n",
    "ax.legend(loc='best')\n",
    "plt.show() \n",
    "plt.savefig('accuracy.pdf')"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "nteract": {
   "version": "0.11.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
